{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"},{"sourceId":6068,"sourceType":"modelInstanceVersion","modelInstanceId":4689}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NLP Disaster Tweets Project","metadata":{}},{"cell_type":"markdown","source":"Our goal is to solve a Natural Language Processing (NLP) problem of predicting whether tweets describe real disasters. \n\nThe challenge of this problem is that people may use words in their tweets that describe a disaster, but are not describing a real disaster. To solve this problem, we use the DistilBert model, which is a pre-trained deep learning model that understands the contextual meaning of text. \n\nWe first preprocessed the text, including conversion to lowercase, word splitting, deactivation, and stemming extraction, and then trained and predicted with the DistilBert model. Finally, we use the confusion matrix to evaluate the performance of the model and submit the predictions to the competition platform. We hope this brief will help you understand our problem and solution.","metadata":{}},{"cell_type":"code","source":"!pip install keras-core --upgrade\n!pip install -q keras-nlp --upgrade\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras_core as keras\nimport keras_nlp\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-26T00:22:21.190155Z","iopub.execute_input":"2024-04-26T00:22:21.190424Z","iopub.status.idle":"2024-04-26T00:23:05.887912Z","shell.execute_reply.started":"2024-04-26T00:22:21.190401Z","shell.execute_reply":"2024-04-26T00:23:05.887053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The selected backend is TensorFlow, while pyTorch can also be selected.","metadata":{}},{"cell_type":"code","source":"os.environ['KERAS_BACKEND'] = 'tensorflow'\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:23:05.889649Z","iopub.execute_input":"2024-04-26T00:23:05.890228Z","iopub.status.idle":"2024-04-26T00:23:05.895710Z","shell.execute_reply.started":"2024-04-26T00:23:05.890198Z","shell.execute_reply":"2024-04-26T00:23:05.894704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA\n\n### Data Importing","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() / 1024**2))","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:23:05.900414Z","iopub.execute_input":"2024-04-26T00:23:05.900675Z","iopub.status.idle":"2024-04-26T00:23:05.991367Z","shell.execute_reply.started":"2024-04-26T00:23:05.900652Z","shell.execute_reply":"2024-04-26T00:23:05.990525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Creating instances of stop words and stemming extractors","metadata":{}},{"cell_type":"code","source":"stop_words = set(stopwords.words('english'))\nps = PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:23:05.992396Z","iopub.execute_input":"2024-04-26T00:23:05.992664Z","iopub.status.idle":"2024-04-26T00:23:06.002394Z","shell.execute_reply.started":"2024-04-26T00:23:05.992640Z","shell.execute_reply":"2024-04-26T00:23:06.001509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pre-processing procedures:\n\n- Changing letters to lower case\n- remove stop words","metadata":{}},{"cell_type":"code","source":"def preprocess_text(text):\n    text = text.lower()\n    words = text.split()\n    words = [ps.stem(word) for word in words if word not in stop_words]\n    text = ' '.join(words)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:23:06.003540Z","iopub.execute_input":"2024-04-26T00:23:06.003940Z","iopub.status.idle":"2024-04-26T00:23:06.010011Z","shell.execute_reply.started":"2024-04-26T00:23:06.003911Z","shell.execute_reply":"2024-04-26T00:23:06.008958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train[\"text\"] = df_train[\"text\"].apply(preprocess_text)\ndf_test[\"text\"] = df_test[\"text\"].apply(preprocess_text)\n\ndf_train[\"length\"] = df_train[\"text\"].apply(lambda x : len(x))\ndf_test[\"length\"] = df_test[\"text\"].apply(lambda x : len(x))\n\nprint(\"Train Length Stat\")\nprint(df_train[\"length\"].describe())\nprint(\"Test Length Stat\")\nprint(df_test[\"length\"].describe())","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:23:06.011346Z","iopub.execute_input":"2024-04-26T00:23:06.011675Z","iopub.status.idle":"2024-04-26T00:23:09.788381Z","shell.execute_reply.started":"2024-04-26T00:23:06.011643Z","shell.execute_reply":"2024-04-26T00:23:09.787454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Visualization","metadata":{}},{"cell_type":"markdown","source":"Histogram of Tweet Length","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Histogram of Tweet Length\nplt.figure(figsize=(10, 6))\nplt.hist(df_train['length'], bins=30, alpha=0.7, color='dodgerblue')\nplt.title('Histogram of Tweet Length')\nplt.xlabel('Length')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:23:09.789674Z","iopub.execute_input":"2024-04-26T00:23:09.790053Z","iopub.status.idle":"2024-04-26T00:23:10.140844Z","shell.execute_reply.started":"2024-04-26T00:23:09.790018Z","shell.execute_reply":"2024-04-26T00:23:10.140000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pie Chart of Disaster vs Non-disaster Tweets","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(6, 6))\ndf_train['target'].value_counts().plot(kind='pie', autopct='%1.1f%%', colors=['skyblue', 'gold'])\nplt.title('Pie Chart of Disaster vs Non-disaster Tweets')\nplt.ylabel('')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:23:10.142091Z","iopub.execute_input":"2024-04-26T00:23:10.142758Z","iopub.status.idle":"2024-04-26T00:23:10.316616Z","shell.execute_reply.started":"2024-04-26T00:23:10.142724Z","shell.execute_reply":"2024-04-26T00:23:10.315354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Average Tweet Length of Disaster vs Non-disaster","metadata":{}},{"cell_type":"code","source":"avg_len_disaster = df_train[df_train['target'] == 1]['length'].mean()\navg_len_non_disaster = df_train[df_train['target'] == 0]['length'].mean()\n\nplt.figure(figsize=(6, 6))\nplt.bar(['Disaster', 'Non-disaster'], [avg_len_disaster, avg_len_non_disaster], color='seagreen')\nplt.title('Average Tweet Length of Disaster vs Non-disaster')\nplt.xlabel('Type')\nplt.ylabel('Average Length')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:23:10.326739Z","iopub.execute_input":"2024-04-26T00:23:10.331054Z","iopub.status.idle":"2024-04-26T00:23:10.533056Z","shell.execute_reply.started":"2024-04-26T00:23:10.331004Z","shell.execute_reply":"2024-04-26T00:23:10.532153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Params\n\nWill use the DistilBert model.","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\nNUM_TRAINING_EXAMPLES = df_train.shape[0]\nTRAIN_SPLIT = 0.8\nVAL_SPLIT = 0.2\nSTEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\nEPOCHS = 3\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:23:10.534287Z","iopub.execute_input":"2024-04-26T00:23:10.534564Z","iopub.status.idle":"2024-04-26T00:23:10.539375Z","shell.execute_reply.started":"2024-04-26T00:23:10.534541Z","shell.execute_reply":"2024-04-26T00:23:10.538468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training\n\n","metadata":{}},{"cell_type":"markdown","source":"DistilBERT is a lightweight BERT that retains most of the performance of BERT, but with a model half the size of BERT and faster training and prediction. Here are some key features of DistilBERT:\n\nTransformer Architecture: DistilBERT is based on the Transformer architecture, a deep learning model architecture that is particularly well suited for processing sequential data, such as text.The key feature of the Transformer architecture is the self-attention mechanism, which allows the model to process each word taking into account all other words in the text.\nPre-training: DistilBERT is a pre-trained model, meaning that it has been pre-trained on large-scale textual data to learn a rich linguistic representation. This enables DistilBERT to understand the contextual meaning of the text and thus efficiently handle various NLP tasks.\nDistillation: The process of creating DistilBERT involves a technique called \"knowledge distillation\". In knowledge distillation, a large model (in this case BERT) is used as a teacher model to instruct a small model (in this case DistilBERT). The small model is trained to mimic the behavior of the teacher model, which allows the small model to learn the knowledge of the teacher model while maintaining a smaller model size and faster training speed.\n\n**MODEL ARCHITECTURE**: The architecture of DistilBERT is very similar to that of BERT in that both include multiple layers of Transformer encoders. Each layer includes a self-attention mechanism and a feed-forward neural network. However, DistilBERT omits some layers in BERT, such as token type embeddings and pooling.\n","metadata":{}},{"cell_type":"markdown","source":"Divide the data into a training set and a validation set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = df_train[\"text\"]\ny = df_train[\"target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n\nX_test = df_test[\"text\"]","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:23:10.540426Z","iopub.execute_input":"2024-04-26T00:23:10.541198Z","iopub.status.idle":"2024-04-26T00:23:10.552617Z","shell.execute_reply.started":"2024-04-26T00:23:10.541166Z","shell.execute_reply":"2024-04-26T00:23:10.551835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use Keras Core, the multi-backend version of Keras for model building.","metadata":{}},{"cell_type":"markdown","source":"Load a DistilBERT model, make the sequences shorter through preprocessor, and finally get a pretrained classifier for final compile and fit process.","metadata":{}},{"cell_type":"code","source":"preset= \"distil_bert_base_en_uncased\"\npreprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset, sequence_length=160, name=\"preprocessor_4_tweets\")\nclassifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,preprocessor = preprocessor,num_classes=2)\n\nclassifier.summary()\n\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    metrics= [\"accuracy\"]  \n)\n\nhistory = classifier.fit(x=X_train,\n                         y=y_train,\n                         batch_size=BATCH_SIZE,\n                         epochs=EPOCHS, \n                         validation_data=(X_val, y_val))","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:23:10.553950Z","iopub.execute_input":"2024-04-26T00:23:10.554586Z","iopub.status.idle":"2024-04-26T00:29:39.853133Z","shell.execute_reply.started":"2024-04-26T00:23:10.554562Z","shell.execute_reply":"2024-04-26T00:29:39.852308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"markdown","source":"We can have a more direct visualization about the model quality.","metadata":{}},{"cell_type":"code","source":"def displayConfusionMatrix(y_true, y_pred, dataset):\n    disp = ConfusionMatrixDisplay.from_predictions(\n        y_true,\n        np.argmax(y_pred, axis=1),\n        display_labels=[\"Not Disaster\",\"Disaster\"],\n        cmap=plt.cm.Blues\n    )\n\n    tn, fp, fn, tp = confusion_matrix(y_true, np.argmax(y_pred, axis=1)).ravel()\n    f1_score = tp / (tp+((fn+fp)/2))\n\n    disp.ax_.set_title(\"Confusion Matrix on \" + dataset + \" Dataset -- F1 Score: \" + str(f1_score.round(2)))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:29:39.854726Z","iopub.execute_input":"2024-04-26T00:29:39.855128Z","iopub.status.idle":"2024-04-26T00:29:39.862361Z","shell.execute_reply.started":"2024-04-26T00:29:39.855081Z","shell.execute_reply":"2024-04-26T00:29:39.861458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = classifier.predict(X_train)\n\ndisplayConfusionMatrix(y_train, y_pred_train, \"Training\")","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:29:39.863658Z","iopub.execute_input":"2024-04-26T00:29:39.864421Z","iopub.status.idle":"2024-04-26T00:30:15.239802Z","shell.execute_reply.started":"2024-04-26T00:29:39.864386Z","shell.execute_reply":"2024-04-26T00:30:15.238886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_val = classifier.predict(X_val)\n\ndisplayConfusionMatrix(y_val, y_pred_val, \"Validation\")","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:30:15.240916Z","iopub.execute_input":"2024-04-26T00:30:15.241235Z","iopub.status.idle":"2024-04-26T00:30:26.314032Z","shell.execute_reply.started":"2024-04-26T00:30:15.241209Z","shell.execute_reply":"2024-04-26T00:30:26.313099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ny_pred_train2 = np.argmax(classifier.predict(X_train), axis=1)\n\naccuracy_train = accuracy_score(y_train, y_pred_train2)\nprecision_train = precision_score(y_train, y_pred_train2)\nrecall_train = recall_score(y_train, y_pred_train2)\nf1_train = f1_score(y_train, y_pred_train2)\n\nprint('Training Accuracy: {:.2f}'.format(accuracy_train))\nprint('Training Precision: {:.2f}'.format(precision_train))\nprint('Training Recall: {:.2f}'.format(recall_train))\nprint('Training F1 Score: {:.2f}'.format(f1_train))\n\ny_pred_val2 = np.argmax(classifier.predict(X_val), axis=1)\n\naccuracy_val = accuracy_score(y_val, y_pred_val2)\nprecision_val = precision_score(y_val, y_pred_val2)\nrecall_val = recall_score(y_val, y_pred_val2)\nf1_val = f1_score(y_val, y_pred_val2)\n\nprint('Validation Accuracy: {:.2f}'.format(accuracy_val))\nprint('Validation Precision: {:.2f}'.format(precision_val))\nprint('Validation Recall: {:.2f}'.format(recall_val))\nprint('Validation F1 Score: {:.2f}'.format(f1_val))\n\n\nmetrics_train = [accuracy_train, precision_train, recall_train, f1_train]\nmetrics_val = [accuracy_val, precision_val, recall_val, f1_val]\n\nlabels = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\nx = np.arange(len(labels))\nwidth = 0.35\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width/2, metrics_train, width, label='Train')\nrects2 = ax.bar(x + width/2, metrics_val, width, label='Validation')\n\nax.set_ylabel('Scores')\nax.set_title('Scores by group and gender')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\nfig.tight_layout()\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:33:15.830895Z","iopub.execute_input":"2024-04-26T00:33:15.831265Z","iopub.status.idle":"2024-04-26T00:33:55.977847Z","shell.execute_reply.started":"2024-04-26T00:33:15.831239Z","shell.execute_reply":"2024-04-26T00:33:55.976997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction Output","metadata":{}},{"cell_type":"markdown","source":"And print the output to file.","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n\nsample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)\n\nsample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-26T00:30:26.315417Z","iopub.execute_input":"2024-04-26T00:30:26.315719Z","iopub.status.idle":"2024-04-26T00:30:45.790949Z","shell.execute_reply.started":"2024-04-26T00:30:26.315693Z","shell.execute_reply":"2024-04-26T00:30:45.789887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nIn this project, we successfully solved a natural language processing (NLP) problem of predicting whether tweets describe real disasters. \n\nWe first performed a series of preprocessing steps on the tweets, including conversion to lowercase, word splitting, deactivation, and stemming extraction, in order to allow the model to better understand the meaning of the text. \n\nWe then trained and predicted using the DistilBert model, a powerful pre-trained deep learning model that understands the contextual meaning of the text to effectively distinguish between tweets describing real disasters and those that are not. \n\nDuring the model training and prediction process, we also use a confusion matrix to evaluate the model's performance and ensure that our model can accurately predict real disaster tweets. \n\nFinally, we completed the project by submitting our predictions to the competition platform. Overall, this project demonstrates how deep learning techniques can be used to solve real-world NLP problems and provides a valuable reference for future related research and applications.\n\nThe selected backend is TensorFlow, while pyTorch can also be selected.","metadata":{}}]}